{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Practical 1**\n",
        "\n",
        "**Aim : Document Indexing and Retrieval**\n",
        "*   Implement an inverted index construction algorithm.\n",
        "*   Build a simple document retrieval system using the constructed index.\n"
      ],
      "metadata": {
        "id": "FBxq1MDO7SnB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyJDo4l06yp0",
        "outputId": "afcc8643-9879-4906-8016-31c850b9d40f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T074 Kermeen\n",
            "Inverted index:\n",
            "\n",
            "lazy -> Document 1 (1), Document 2 (1), \n",
            "quick -> Document 1 (1), \n",
            "fox -> Document 1 (1), \n",
            "slept -> Document 2 (1), \n",
            "dog -> Document 1 (1), Document 2 (1), \n",
            "jumped -> Document 1 (1), \n",
            "brown -> Document 1 (1), \n",
            "sun -> Document 2 (1), \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "print(\"T074 Kermeen\")\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#Define the documents\n",
        "document1= \"The quick brown fox jumped over the lazy dog\"\n",
        "document2=\"The lazy dog slept in the sun\"\n",
        "\n",
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "stopWords= stopwords.words('english')\n",
        "\n",
        "tokens1=document1.lower().split()\n",
        "tokens2=document2.lower().split()\n",
        "\n",
        "terms=list(set(tokens1+tokens2))\n",
        "\n",
        "inverted_index={}\n",
        "occ_num_doc1={}\n",
        "occ_num_doc2={}\n",
        "\n",
        "for term in terms:\n",
        "    if term in stopWords:\n",
        "        continue\n",
        "    documents=[]\n",
        "\n",
        "    if term in tokens1:\n",
        "        documents.append(\"Document 1\")\n",
        "        occ_num_doc1[term]=tokens1.count(term)\n",
        "\n",
        "    if term in tokens2:\n",
        "        documents.append(\"Document 2\")\n",
        "        occ_num_doc2[term]=tokens2.count(term)\n",
        "    inverted_index[term]=documents\n",
        "print(\"Inverted index:\\n\")\n",
        "\n",
        "for term,docs in inverted_index.items():\n",
        "    print(term, \"->\",end=\" \")\n",
        "\n",
        "    for doc in docs:\n",
        "        if doc ==\"Document 1\":\n",
        "            print(f\"{doc} ({occ_num_doc1.get(term,0)}),\", end=\" \")\n",
        "        else:\n",
        "            print(f\"{doc} ({occ_num_doc2.get(term,0)}),\", end=\" \")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1**\n",
        "\n",
        "Implement an inverted index construction algorithm for the following two documents.\n",
        "\n",
        "document1 = \"The computer science students are appearing for practical examination.\"\n",
        "\n",
        "document2 = \"computer science practical examination will start tomorrow.\"\n",
        "\n",
        "Build a simple document retrieval system using the constructed index to find the\n",
        "documents containing terms “computer science”."
      ],
      "metadata": {
        "id": "ZaJSwzg48dgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"T074 Kermeen\")\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "document1 = \"The computer science students are appearing for practical examination.\"\n",
        "document2 = \"computer science practical examination will start tomorrow.\"\n",
        "nltk.download('stopwords')\n",
        "stopWords = stopwords.words('english')\n",
        "\n",
        "tokens1 = document1.lower().split()\n",
        "tokens2 = document2.lower().split()\n",
        "\n",
        "terms = list(set(tokens1 + tokens2))\n",
        "\n",
        "inverted_index = {}\n",
        "occ_num_doc1 = {}\n",
        "occ_num_doc2 = {}\n",
        "\n",
        "for term in terms:\n",
        "    if term in stopWords:\n",
        "        continue  # ignore stopwords\n",
        "    documents = []\n",
        "    if term in tokens1:\n",
        "        documents.append(\"Document 1\")\n",
        "        occ_num_doc1[term] = tokens1.count(term)\n",
        "\n",
        "    if term in tokens2:\n",
        "        documents.append(\"Document 2\")\n",
        "        occ_num_doc2[term] = tokens2.count(term)\n",
        "\n",
        "    inverted_index[term] = documents\n",
        "\n",
        "query = [\"computer\", \"science\"]   # query terms\n",
        "\n",
        "print(\"\\nDocuments containing the terms 'computer science':\\n\")\n",
        "result_docs = set(inverted_index.get(query[0], []))\n",
        "for term in query[1:]:\n",
        "    result_docs = result_docs.intersection(inverted_index.get(term, []))\n",
        "if result_docs:\n",
        "    print(\"Retrieved Documents:\", result_docs)\n",
        "else:\n",
        "    print(\"No document contains all query terms.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAHHN8Ms8m23",
        "outputId": "ad3f48d4-5e0b-4faa-c82e-b38c0232c87e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T074 Kermeen\n",
            "\n",
            "Documents containing the terms 'computer science':\n",
            "\n",
            "Retrieved Documents: {'Document 2', 'Document 1'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2**\n",
        "\n",
        "Implement an inverted index construction algorithm for the following two documents.\n",
        "\n",
        "document1 = \"today is a beautiful and a sunny day\"\n",
        "\n",
        "document2 = \"it was a cloudy day\"\n",
        "\n",
        "Build a simple document retrieval system using the constructed index to find the\n",
        "documents containing terms “beautiful day”."
      ],
      "metadata": {
        "id": "GaGr1UtJ8nNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"T074 Kermeen\")\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "document1 = \"today is a beautiful and a sunny day\"\n",
        "document2 = \"it was a cloudy day\"\n",
        "nltk.download('stopwords')\n",
        "stopWords = stopwords.words('english')\n",
        "\n",
        "tokens1 = document1.lower().split()\n",
        "tokens2 = document2.lower().split()\n",
        "\n",
        "# Unique list of terms\n",
        "terms = list(set(tokens1 + tokens2))\n",
        "\n",
        "# Inverted index and occurrences\n",
        "inverted_index = {}\n",
        "occ_num_doc1 = {}\n",
        "occ_num_doc2 = {}\n",
        "\n",
        "# Build inverted index\n",
        "for term in terms:\n",
        "    if term in stopWords:\n",
        "        continue  # ignore stopwords\n",
        "\n",
        "    documents = []\n",
        "    if term in tokens1:\n",
        "        documents.append(\"Document 1\")\n",
        "        occ_num_doc1[term] = tokens1.count(term)\n",
        "\n",
        "    if term in tokens2:\n",
        "        documents.append(\"Document 2\")\n",
        "        occ_num_doc2[term] = tokens2.count(term)\n",
        "    inverted_index[term] = documents\n",
        "query = [\"beautiful\", \"day\"]   # query terms\n",
        "print(\"\\nDocuments containing the terms 'beautiful day':\\n\")\n",
        "result_docs = set(inverted_index.get(query[0], []))\n",
        "for term in query[1:]:\n",
        "    result_docs = result_docs.intersection(inverted_index.get(term, []))\n",
        "if result_docs:\n",
        "    print(\"Retrieved Documents:\", result_docs)\n",
        "else:\n",
        "    print(\"No document contains all query terms.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6-lyFqY8tQX",
        "outputId": "2698046f-9056-4d50-85c9-fb400e351027"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T074 Kermeen\n",
            "\n",
            "Documents containing the terms 'beautiful day':\n",
            "\n",
            "Retrieved Documents: {'Document 1'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3**\n",
        "\n",
        "Implement an inverted index construction algorithm for the following two documents\n",
        "\n",
        "document1 = \"The quick brown fox jumped over the lazy dog\"\n",
        "\n",
        "document2 = \"The lazy dog slept in the sun\"\n",
        "\n",
        "Build a simple document retrieval system using the constructed index to\n",
        "find the documents containing terms “lazy sun”"
      ],
      "metadata": {
        "id": "SQ4rTs--8tfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"T074 Kermeen\")\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Define the documents\n",
        "document1 = \"The quick brown fox jumped over the lazy dog\"\n",
        "document2 = \"The lazy dog slept in the sun\"\n",
        "\n",
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "stopWords = stopwords.words('english')\n",
        "\n",
        "# Tokenize\n",
        "tokens1 = document1.lower().split()\n",
        "tokens2 = document2.lower().split()\n",
        "terms = list(set(tokens1 + tokens2))\n",
        "\n",
        "inverted_index = {}\n",
        "occ_num_doc1 = {}\n",
        "occ_num_doc2 = {}\n",
        "for term in terms:\n",
        "    if term in stopWords:\n",
        "        continue  # ignore stopwords\n",
        "    documents = []\n",
        "    if term in tokens1:\n",
        "        documents.append(\"Document 1\")\n",
        "        occ_num_doc1[term] = tokens1.count(term)\n",
        "\n",
        "    if term in tokens2:\n",
        "        documents.append(\"Document 2\")\n",
        "        occ_num_doc2[term] = tokens2.count(term)\n",
        "\n",
        "    inverted_index[term] = documents\n",
        "\n",
        "query = [\"lazy\", \"sun\"]   # query terms\n",
        "\n",
        "print(\"\\nDocuments containing the terms 'lazy sun':\\n\")\n",
        "result_docs = set(inverted_index.get(query[0], []))\n",
        "\n",
        "for term in query[1:]:\n",
        "    result_docs = result_docs.intersection(inverted_index.get(term, []))\n",
        "\n",
        "if result_docs:\n",
        "    print(\"Retrieved Documents:\", result_docs)\n",
        "else:\n",
        "    print(\"No document contains all query terms.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30HJXPYT8yaX",
        "outputId": "0c06a27a-2455-4077-81bd-c7135b0e821f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T074 Kermeen\n",
            "\n",
            "Documents containing the terms 'lazy sun':\n",
            "\n",
            "Retrieved Documents: {'Document 2'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4**\n",
        "\n",
        "Implement an inverted index construction algorithm for the following two documents\n",
        "\n",
        "document1 = \"best of luck tycs students for your practical examination.\"\n",
        "\n",
        "document2 = \"tycs students please carry your journal at the time of practical examination.\"\n",
        "\n",
        "Build a simple document retrieval system using the constructed index to find the documents containing terms “tycs journal”"
      ],
      "metadata": {
        "id": "BxN7FUkE80VH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"T074 Kermeen\")\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "document1 = \"best of luck tycs students for your practical examination.\"\n",
        "document2 = \"tycs students please carry your journal at the time of practical examination.\"\n",
        "nltk.download('stopwords')\n",
        "stopWords = stopwords.words('english')\n",
        "\n",
        "tokens1 = document1.lower().split()\n",
        "tokens2 = document2.lower().split()\n",
        "\n",
        "terms = list(set(tokens1 + tokens2))\n",
        "inverted_index = {}\n",
        "occ_num_doc1 = {}\n",
        "occ_num_doc2 = {}\n",
        "for term in terms:\n",
        "    if term in stopWords:\n",
        "        continue  # ignore stopwords\n",
        "    documents = []\n",
        "    if term in tokens1:\n",
        "        documents.append(\"Document 1\")\n",
        "        occ_num_doc1[term] = tokens1.count(term)\n",
        "    if term in tokens2:\n",
        "        documents.append(\"Document 2\")\n",
        "        occ_num_doc2[term] = tokens2.count(term)\n",
        "\n",
        "    inverted_index[term] = documents\n",
        "query = [\"tycs\", \"journal\"]   # query terms\n",
        "print(\"\\nDocuments containing the terms 'tycs journal':\\n\")\n",
        "result_docs = set(inverted_index.get(query[0], []))\n",
        "for term in query[1:]:\n",
        "    result_docs = result_docs.intersection(inverted_index.get(term, []))\n",
        "if result_docs:\n",
        "    print(\"Retrieved Documents:\", result_docs)\n",
        "else:\n",
        "    print(\"No document contains all query terms.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wHel9MV88kf",
        "outputId": "55afb9f0-dc26-44fc-bc7b-844167d38211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T074 Kermeen\n",
            "\n",
            "Documents containing the terms 'tycs journal':\n",
            "\n",
            "Retrieved Documents: {'Document 2'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5**\n",
        "\n",
        "Implement an inverted index construction algorithm for the following two documents\n",
        "\n",
        "document1 = \"our class meeting starts soon\"\n",
        "\n",
        "document2 = \"my class starts at 6.\"\n",
        "\n",
        "Build a simple document retrieval system using the constructed index to\n",
        "find the documents containing terms “class meeting”."
      ],
      "metadata": {
        "id": "aiXUXSqnBPjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"T074 Kermeen\")\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "document1 = \"our class meeting starts soon\"\n",
        "document2 = \"my class starts at 6.\"\n",
        "nltk.download('stopwords')\n",
        "stopWords = stopwords.words('english')\n",
        "\n",
        "tokens1 = document1.lower().split()\n",
        "tokens2 = document2.lower().split()\n",
        "\n",
        "terms = list(set(tokens1 + tokens2))\n",
        "inverted_index = {}\n",
        "occ_num_doc1 = {}\n",
        "occ_num_doc2 = {}\n",
        "for term in terms:\n",
        "    if term in stopWords:\n",
        "        continue  # ignore stopwords\n",
        "    documents = []\n",
        "    if term in tokens1:\n",
        "        documents.append(\"Document 1\")\n",
        "        occ_num_doc1[term] = tokens1.count(term)\n",
        "    if term in tokens2:\n",
        "        documents.append(\"Document 2\")\n",
        "        occ_num_doc2[term] = tokens2.count(term)\n",
        "\n",
        "    inverted_index[term] = documents\n",
        "query = [\"tycs\", \"journal\"]   # query terms\n",
        "print(\"\\nDocuments containing the terms 'tycs journal':\\n\")\n",
        "result_docs = set(inverted_index.get(query[0], []))\n",
        "for term in query[1:]:\n",
        "    result_docs = result_docs.intersection(inverted_index.get(term, []))\n",
        "if result_docs:\n",
        "    print(\"Retrieved Documents:\", result_docs)\n",
        "else:\n",
        "    print(\"No document contains all query terms.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55afb9f0-dc26-44fc-bc7b-844167d38211",
        "id": "0LTSsjDPBXiB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T074 Kermeen\n",
            "\n",
            "Documents containing the terms 'tycs journal':\n",
            "\n",
            "Retrieved Documents: {'Document 2'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}